{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import keras\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(y, sr):\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    return {\n",
    "        'chroma_stft_mean': np.mean(chroma_stft),\n",
    "        'chroma_stft_std': np.std(chroma_stft),\n",
    "        'spectral_centroid_mean': np.mean(spectral_centroid),\n",
    "        'spectral_centroid_std': np.std(spectral_centroid),\n",
    "        'spectral_bandwidth_mean': np.mean(spectral_bandwidth),\n",
    "        'spectral_bandwidth_std': np.std(spectral_bandwidth),\n",
    "        'rolloff_mean': np.mean(rolloff),\n",
    "        'rolloff_std': np.std(rolloff),\n",
    "        'zero_crossing_rate_mean': np.mean(zero_crossing_rate),\n",
    "        'zero_crossing_rate_std': np.std(zero_crossing_rate),\n",
    "        **{f'mfcc{i}_mean': np.mean(mfcc[i]) for i in range(mfcc.shape[0])},\n",
    "        **{f'mfcc{i}_std': np.std(mfcc[i]) for i in range(mfcc.shape[0])}\n",
    "    }\n",
    "from joblib import dump, load\n",
    "rf_model = load(r'C:\\Users\\hp\\Desktop\\happy\\rf_model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "# --- PyAudio Configuration ---\n",
    "CHUNK_SIZE = 1024  # Adjust based on your system and latency needs\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1  # If using a mono microphone\n",
    "RATE = 48000  # Or adjust to match your microphone's sampling rate\n",
    "# --- Initialize Audio Stream ---\n",
    "p = pyaudio.PyAudio()   \n",
    "stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                input=True, frames_per_buffer=CHUNK_SIZE,)\n",
    "\n",
    "# --- Circular Buffer (Simplified for Example) ---\n",
    "buffer = []  \n",
    "BUFFER_LENGTH = 100  # Adjust this to control analysis length\n",
    "\n",
    "# --- Main Loop ---\n",
    "try:\n",
    "    while True:\n",
    "        data = stream.read(CHUNK_SIZE)\n",
    "        buffer.append(data)\n",
    "        \n",
    "    if len(buffer) == BUFFER_LENGTH:\n",
    "            # Extract audio data from the buffer\n",
    "            frames = b''.join(buffer) \n",
    "            y = np.frombuffer(frames, dtype=np.int16).astype(np.float32) \n",
    "\n",
    "            # Extract features \n",
    "            features = extract_features(y, RATE) \n",
    "            df = pd.DataFrame(features, index=[0])\n",
    "\n",
    "            # Make prediction\n",
    "            prediction = rf_model.predict(df)[0]\n",
    "            print(\"Predicted emotion:\", prediction)\n",
    "\n",
    "            buffer = []  # Clear the buffer\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting...\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "p.terminate()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 16:49:52.528 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\hp\\Desktop\\happy\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "import pyaudio\n",
    "\n",
    "# --- PyAudio Configuration ---\n",
    "CHUNK_SIZE = 1024  # Adjust based on your system and latency needs\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1  # If using a mono microphone\n",
    "RATE = 48000  # Or adjust to match your microphone's sampling rate\n",
    "\n",
    "# --- Feature Extraction Function ---\n",
    "def extract_features(y, sr):\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "\n",
    "    return {\n",
    "        'chroma_stft_mean': np.mean(chroma_stft),\n",
    "        'chroma_stft_std': np.std(chroma_stft),\n",
    "        'spectral_centroid_mean': np.mean(spectral_centroid),\n",
    "        'spectral_centroid_std': np.std(spectral_centroid),\n",
    "        'spectral_bandwidth_mean': np.mean(spectral_bandwidth),\n",
    "        'spectral_bandwidth_std': np.std(spectral_bandwidth),\n",
    "        'rolloff_mean': np.mean(rolloff),\n",
    "        'rolloff_std': np.std(rolloff),\n",
    "        'zero_crossing_rate_mean': np.mean(zero_crossing_rate),\n",
    "        'zero_crossing_rate_std': np.std(zero_crossing_rate),\n",
    "        **{f'mfcc{i}_mean': np.mean(mfcc[i]) for i in range(mfcc.shape[0])},\n",
    "        **{f'mfcc{i}_std': np.std(mfcc[i]) for i in range(mfcc.shape[0])}\n",
    "    }\n",
    "\n",
    "# --- Load Pre-trained Model ---\n",
    "try:\n",
    "    rf_model = load(\"finalized_model.sav.joblib\")\n",
    "    st.success(\"Pre-trained emotion detection model loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    st.error(\"Error: 'finalized_model.sav.joblib' not found. Please ensure the model is in the same directory.\")\n",
    "    st.stop()  # Halt execution if model not found\n",
    "\n",
    "# --- Circular Buffer (Simplified for Streamlit Integration) ---\n",
    "buffer = []\n",
    "BUFFER_LENGTH = 100  # Adjust this to control analysis length\n",
    "\n",
    "# --- Main Function (Called by Streamlit) ---\n",
    "def predict_emotion():\n",
    "    global buffer\n",
    "\n",
    "    try:\n",
    "        # Initialize PyAudio stream (created within the function for continuous capture)\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                        input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "        while True:\n",
    "            data = stream.read(CHUNK_SIZE)\n",
    "            buffer.append(data)\n",
    "\n",
    "            if len(buffer) == BUFFER_LENGTH:\n",
    "                # Extract audio data from the buffer\n",
    "                frames = b''.join(buffer)\n",
    "                y = np.frombuffer(frames, dtype=np.int16).astype(np.float32)\n",
    "\n",
    "                # Extract features\n",
    "                features = extract_features(y, RATE)\n",
    "                df = pd.DataFrame(features, index=[0])\n",
    "\n",
    "                # Make prediction\n",
    "                prediction = rf_model.predict(df)[0]\n",
    "\n",
    "                # Display prediction on Streamlit interface\n",
    "                st.write(f\"Predicted Emotion: {prediction}\")\n",
    "                buffer = []  # Clear the buffer\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting...\")\n",
    "    finally:\n",
    "        # Close audio stream resources\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "# --- Streamlit App ---\n",
    "st.title(\"Real-Time Emotion Detection\")\n",
    "st.write(\"Speak into your microphone and see the predicted emotion!\")\n",
    "\n",
    "# Start emotion prediction on button click\n",
    "if st.button(\"Start Prediction\"):\n",
    "    predict_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
